# ğŸ“° News Fetcher

ä¸€å€‹è‡ªå‹•æŠ“å–æ–°è RSSã€é€²è¡Œç¶²é çˆ¬èŸ²ã€å­˜å…¥ MongoDBï¼Œä¸¦åŸ·è¡Œæƒ…æ„Ÿåˆ†æèˆ‡å»é‡æ¯”å°çš„ç¨‹å¼ã€‚  
æœ¬å°ˆæ¡ˆä½¿ç”¨ **Docker Compose** éƒ¨ç½²ï¼ŒåŒ…å«ä¸‰å€‹æœå‹™ï¼š
- **app**ï¼šPython çˆ¬èŸ²èˆ‡åˆ†æç¨‹å¼
- **mongo**ï¼šMongoDB è³‡æ–™åº«
- **mongo-express**ï¼šWeb ç®¡ç†ä»‹é¢

---

## ğŸš€ å®‰è£æµç¨‹

### 1. å‰ç½®éœ€æ±‚
- å®‰è£ [Docker](https://docs.docker.com/get-docker/) (Engine 20+ æˆ– Docker Desktop æœ€æ–°ç‰ˆ)  
- å®‰è£ [Docker Compose](https://docs.docker.com/compose/install/) (v2 ä»¥ä¸Š)

æª¢æŸ¥ï¼š
```bash
docker --version
docker compose version
```

---

### 2. å–å¾—å°ˆæ¡ˆ
è§£å£“ç¸®å°ˆæ¡ˆ zipï¼š
```bash
unzip news-fetcher.zip
cd news-fetcher
```

å°ˆæ¡ˆçµæ§‹ï¼š
```
news-fetcher/
â”œâ”€ Dockerfile
â”œâ”€ docker-compose.yml
â”œâ”€ requirements.txt
â”œâ”€ main.py
â”œâ”€ parsing/
â”‚  â”œâ”€ html_scraper.py
â”‚  â”œâ”€ sentiment_analysis.py
â”‚  â””â”€ deduplication.py
â””â”€ data/
   â””â”€ keyword_filter_set_zh.csv
```

---

### 3. è¨­å®šç’°å¢ƒè®Šæ•¸
å»ºç«‹ `.env` æª”æ¡ˆï¼Œåƒ…éœ€è¨­å®š MongoDB å¸³å¯†ï¼š

```
MONGO_INITDB_ROOT_USERNAME=root
MONGO_INITDB_ROOT_PASSWORD=example-strong-password
```

> âš ï¸ ä¸éœ€è¦è¨­å®š `OPENAI_API_KEY`ï¼Œå®ƒå·²ç¶“å¯«æ­»åœ¨ `docker-compose.yml`ã€‚  
> âš ï¸ å»ºè­°å°‡ Mongo å¯†ç¢¼æ›æˆå®‰å…¨å­—ä¸²ã€‚  

---

### 4. å»ºç½®èˆ‡å•Ÿå‹•
ç¬¬ä¸€æ¬¡å•Ÿå‹•éœ€å…ˆå»ºç½®ï¼š
```bash
docker compose build
docker compose up -d
```

é€™æœƒå•Ÿå‹•ä¸‰å€‹æœå‹™ï¼š
- `mongo` â†’ MongoDB
- `app` â†’ çˆ¬èŸ²ç¨‹å¼
- `mongo-express` â†’ Web ç®¡ç†ä»‹é¢ (http://localhost:8081)

---

## ğŸ” ä½¿ç”¨æ–¹æ³•

### æŸ¥çœ‹å®¹å™¨ç‹€æ…‹
```bash
docker compose ps
```

### æŸ¥çœ‹ç¨‹å¼æ—¥èªŒ
```bash
docker compose logs -f app
```

### æ‰‹å‹•è§¸ç™¼æŠ“å–
```bash
docker exec -it news-fetcher python main.py
```

### é€²å…¥ Mongo CLI
```bash
docker exec -it mongo mongosh -u root -p <ä½ çš„å¯†ç¢¼> --authenticationDatabase admin
```

æŸ¥è©¢ç¯„ä¾‹ï¼š
```javascript
use news_info
db.article_info.countDocuments()
db.article_info.findOne()
```

### ä½¿ç”¨ Web ç®¡ç†ä»‹é¢
æ‰“é–‹ç€è¦½å™¨ï¼š
ğŸ‘‰ http://localhost:8081  
å³å¯é€éåœ–å½¢åŒ–ä»‹é¢æŸ¥çœ‹è³‡æ–™ã€‚

---

## âš™ï¸ æ—¥å¸¸æ“ä½œ

- å•Ÿå‹•  
  ```bash
  docker compose up -d
  ```
- åœæ­¢  
  ```bash
  docker compose down
  ```
- é‡å»ºï¼ˆä¿®æ”¹äº† Dockerfile æˆ– requirementsï¼‰  
  ```bash
  docker compose build app
  docker compose up -d
  ```
- åªé‡å•Ÿ App  
  ```bash
  docker compose restart app
  ```

---

## ğŸ’¾ è³‡æ–™ä¿å­˜
- MongoDB è³‡æ–™å­˜åœ¨ volume `mongo-data` ä¸­ï¼Œåˆªé™¤å®¹å™¨ä¸æœƒæ¶ˆå¤±ã€‚  
- è‹¥è¦æ¸…ç©ºï¼ˆâš ï¸æœƒåˆªå…‰æ‰€æœ‰è³‡æ–™ï¼‰ï¼š  
  ```bash
  docker compose down -v
  ```

---

## âœ… ç¸½çµ
1. å®‰è£ Docker + Compose  
2. è§£å£“ç¸®å°ˆæ¡ˆä¸¦é€²å…¥ç›®éŒ„  
3. å»ºç«‹ `.env`ï¼ˆåªæ”¾ Mongo å¸³å¯†ï¼Œä¸éœ€ OpenAI Keyï¼‰  
4. `docker compose build && docker compose up -d`  
5. æ‰“é–‹ http://localhost:18081 æˆ–ç”¨ `mongosh` æŸ¥è³‡æ–™  

---


## Description

News Agent will scrape 350 articles at 3 scheduled times a day (07:30, 13:30, 18:00  UTC+8) from a list of sources. The sources are split into high priority and low priority categories, and the high priority list is fetched first. The low priority list will be fetched after, and the rest of the articles needed to meet the 350 quota will be scraped from PTT Stock Board. The articles are checked for keywords and skipped if no keywords are found, and they are also checked against the articles already in the database to see if there is already a duplicate in the database. If a duplicate is found, the article is skipped. Information from these articles are stored in the "article_info" collection in the database. After parsing, the articles go through deduplication through TF-IDF or SBERT vectorization and cosine similarity. Most exact duplicates should have already been skipped, so this step would be for removing articles on similar topics. There is a threshold parameter to control what similarity score is considered too high, which allows for customization of how similar articles must be for removal. After deduplication, the articles go through sentiment analysis via concurrent calls to OpenAI. This creates three documents for each article: one for general sentiment analysis, one for detail on the confidence score given, and one for entity matches. These are each stored in a separate collection, and each parsed article is linked to its three connecting analysis files by the "id" field in each document.    
Python Version 3.11.9

## Components

main.py handles the scheduling of runs and all RSS parsing, as well as calling functions from other modules.  
Parsing folder contains the following modules:  
- html_scraper.py: handles all html parsing using BeautifulSoup  
- deduplication.py: handles deduplication via tf-idf or sbert vectorization and cosine similarity.  
- sentiment_analysis.py: gets responses from OpenAI to analyze the news scraped from main.py

## Configurables:

- deduplication.pyâ€” tfidf_comparison /sbert_comparisonâ€” either can be utilized
- deduplication.py â€” tfidf_comparison/sbert_comparisonâ€”threshold parameter can be changed when the function is called depending on what similarity score is considered â€œtoo similarâ€
- html_scraper.pyâ€”find_text_by_name/find_text_by_id/find_text_by_classâ€”can all be utilized to find the body text in p tags within the tag identified by name, id, or class.
- main.pyâ€”connection_stringâ€”replace with the connection string of the desired MongoDB database
- main.pyâ€”database_nameâ€”replace with the name of the desired database
- sentiment_analysis.pyâ€”async_analyzeâ€”will require OpenAI key, currently is using a key saved to my environment

## Testing:

- **main.py** â€” To test: run the daily_fetch() function, errors/time taken for each step are logged. â€” Expected result: fetches information from 350 articles each run, 3 runs scheduled per dayâ€”Note: the functions in main.py rely on other modules (deduplication.py, sentiment_analysis.py, html_scraper.py)
- **html_scraper.py**â€”To test: run the file (has logic to run on its own)â€”Expected result: PTT scraping functions gather information from a specific amount of articles and stores in MongoDB. Functions to get body text can be tested with the fetch() function in main.py.
- **deduplication.py**â€”To test: run the file (has logic to run on its own)â€” Expected result: Removes duplicates from a MongoDB collection. Titles of deleted articles are logged and can be used to determine if the deduplication logic is running properly.
- **sentiment_analysis.py**â€”To test: run the file (has logic to run on its own)â€”Expected result: gets OpenAI responses to analyze all documents from the specified start index, stores 3 types of results (entity match, sentiment analysis, confidence score detail) in 3 different collections.
